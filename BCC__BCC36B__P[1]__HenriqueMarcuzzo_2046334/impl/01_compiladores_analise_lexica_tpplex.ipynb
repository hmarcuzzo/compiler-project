{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-compiladores-analise-lexica-tpplex.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "a-s-dVnxdbZW",
        "outputId": "6a45e2e7-515c-4408-b0df-8b609d495145"
      },
      "source": [
        "%%javascript\n",
        "require(\"base/js/utils\").load_extensions(\"highlighter/highlighter\")"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Javascript object>",
            "application/javascript": "require(\"base/js/utils\").load_extensions(\"highlighter/highlighter\")\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGxUtELATG9J"
      },
      "source": [
        "from sys import argv, exit\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "     level = logging.DEBUG,\n",
        "     filename = \"log.txt\",\n",
        "     filemode = \"w\",\n",
        "     format = \"%(filename)10s:%(lineno)4d:%(message)s\"\n",
        ")\n",
        "log = logging.getLogger()\n",
        "\n",
        "import ply.lex as lex\n",
        "from ply.lex import TOKEN"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V2XZckUT6vh",
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        "tokens = [\n",
        "    \"ID\",  # identificador\n",
        "    # numerais\n",
        "    \"NUM_NOTACAO_CIENTIFICA\",  # ponto flutuante em notaçao científica\n",
        "    \"NUM_PONTO_FLUTUANTE\",  # ponto flutuate\n",
        "    \"NUM_INTEIRO\",  # inteiro\n",
        "    # operadores binarios\n",
        "    \"ADICAO\",  # +\n",
        "    \"SUBTRACAO\",  # -\n",
        "    \"MULTIPLICACAO\",  # *\n",
        "    \"DIVISAO\",  # /\n",
        "    \"E_LOGICO\",  # &&\n",
        "    \"OU_LOGICO\",  # ||\n",
        "    \"DIFERENCA\",  # <>\n",
        "    \"MENOR_IGUAL\",  # <=\n",
        "    \"MAIOR_IGUAL\",  # >=\n",
        "    \"MENOR\",  # <\n",
        "    \"MAIOR\",  # >\n",
        "    \"IGUALDADE\",  # =\n",
        "    # operadores unarios\n",
        "    \"NEGACAO\",  # !\n",
        "    # simbolos\n",
        "    \"ABRE_PAR\",  # (\n",
        "    \"FECHA_PAR\",  # )\n",
        "    \"ABRE_COL\",  # [\n",
        "    \"FECHA_COL\",  # ]\n",
        "    \"VIRGULA\",  # ,\n",
        "    \"DOIS_PONTOS\",  # :\n",
        "    \"ATRIBUICAO\",  # :=\n",
        "    # 'COMENTARIO', # {***}\n",
        "]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ5adkC1T_dg"
      },
      "source": [
        "reserved_words = {\n",
        "    \"se\": \"SE\",\n",
        "    \"então\": \"ENTAO\",\n",
        "    \"senão\": \"SENAO\",\n",
        "    \"fim\": \"FIM\",\n",
        "    \"repita\": \"REPITA\",\n",
        "    \"flutuante\": \"FLUTUANTE\",\n",
        "    \"retorna\": \"RETORNA\",\n",
        "    \"até\": \"ATE\",\n",
        "    \"leia\": \"LEIA\",\n",
        "    \"escreva\": \"ESCREVA\",\n",
        "    \"inteiro\": \"INTEIRO\",\n",
        "}\n",
        "\n",
        "tokens = tokens + list(reserved_words.values())"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px_JLT-mUEqx"
      },
      "source": [
        "digito = r\"([0-9])\"\n",
        "letra = r\"([a-zA-ZáÁãÃàÀéÉíÍóÓõÕ])\"\n",
        "sinal = r\"([\\-\\+]?)\"\n",
        "\n",
        "\"\"\" \n",
        "    id deve começar com uma letra\n",
        "\"\"\"\n",
        "id = (\n",
        "    r\"(\" + letra + r\"(\" + digito + r\"+|_|\" + letra + r\")*)\"\n",
        ")  # o mesmo que '((letra)(letra|_|([0-9]))*)'\n",
        "\n",
        "inteiro = r\"(\" + sinal + digito + r\"+)\"\n",
        "\n",
        "flutuante = (\n",
        "    # r\"(\" + digito + r\"+\\.\" + digito + r\"+?)\"\n",
        "    # (([-\\+]?)([0-9]+)\\.([0-9]+))'\n",
        "    r'\\d+[eE][-+]?\\d+|(\\.\\d+|\\d+\\.\\d*)([eE][-+]?\\d+)?'\n",
        "    # r'[-+]?[0-9]+(\\.([0-9]+)?)'\n",
        "    #r'[+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?'\n",
        "    #r\"(([-\\+]?)([0-9]+)\\.([0-9]+))\"\n",
        "    )\n",
        "\n",
        "notacao_cientifica = (\n",
        "    r\"(\" + sinal + r\"([1-9])\\.\" + digito + r\"+[eE]\" + sinal + digito + r\"+)\"\n",
        ")  # o mesmo que '(([-\\+]?)([1-9])\\.([0-9])+[eE]([-\\+]?)([0-9]+))'"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuIxrFzjUIY8"
      },
      "source": [
        " \n",
        "# Expressões Regulaes para tokens simples.\n",
        "# Símbolos.\n",
        "t_ADICAO    = r'\\+'\n",
        "t_SUBTRACAO  = r'-'\n",
        "t_MULTIPLICACAO   = r'\\*'\n",
        "t_DIVISAO = r'/'\n",
        "t_ABRE_PAR  = r'\\('\n",
        "t_FECHA_PAR  = r'\\)'\n",
        "t_ABRE_COL = r'\\['\n",
        "t_FECHA_COL = r'\\]'\n",
        "t_VIRGULA = r','\n",
        "t_ATRIBUICAO = r':='\n",
        "t_DOIS_PONTOS = r':'\n",
        "\n",
        "# Operadores Lógicos.\n",
        "t_E_LOGICO = r'&&'\n",
        "t_OU_LOGICO = r'\\|\\|'\n",
        "t_NEGACAO = r'!'\n",
        "\n",
        "# Operadores Relacionais.\n",
        "t_DIFERENCA = r'<>'\n",
        "t_MENOR_IGUAL = r'<='\n",
        "t_MAIOR_IGUAL = r'>='\n",
        "t_MENOR = r'<'\n",
        "t_MAIOR = r'>'\n",
        "t_IGUALDADE = r'='"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-gnMlvxUL00"
      },
      "source": [
        "@TOKEN(id)\n",
        "def t_ID(token):\n",
        "    token.type = reserved_words.get(\n",
        "        token.value, \"ID\"\n",
        "    )  # não é necessário fazer regras/regex para cada palavra reservada\n",
        "    # se o token não for uma palavra reservada automaticamente é um id\n",
        "    # As palavras reservadas têm precedências sobre os ids\n",
        "\n",
        "    return token\n",
        "\n",
        "@TOKEN(notacao_cientifica)\n",
        "def t_NUM_NOTACAO_CIENTIFICA(token):\n",
        "    return token\n",
        "\n",
        "@TOKEN(flutuante)\n",
        "def t_NUM_PONTO_FLUTUANTE(token):\n",
        "    return token\n",
        "\n",
        "@TOKEN(inteiro)\n",
        "def t_NUM_INTEIRO(token):\n",
        "    return token"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPVsN3naUO0W"
      },
      "source": [
        "t_ignore = \" \\t\"\n",
        "\n",
        "# t_COMENTARIO = r'(\\{((.|\\n)*?)\\})'\n",
        "# para poder contar as quebras de linha dentro dos comentarios\n",
        "def t_COMENTARIO(token):\n",
        "    # r\"(\\{((.|\\n)*?)\\})\"\n",
        "    r\"(\\\\)|(\\{((.|\\n)*?)\\})\"\n",
        "    token.lexer.lineno += token.value.count(\"\\n\")\n",
        "    # return token\n",
        "\n",
        "def t_newline(token):\n",
        "    r\"\\n+\"\n",
        "    token.lexer.lineno += len(token.value)\n",
        "\n",
        "def define_column(input, lexpos):\n",
        "    begin_line = input.rfind(\"\\n\", 0, lexpos) + 1\n",
        "    return (lexpos - begin_line) + 1"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC49Ezx-USJ7"
      },
      "source": [
        "def t_error(token):\n",
        "\n",
        "    # file = token.lexer.filename\n",
        "    line = token.lineno\n",
        "    # column = define_column(token.lexer.backup_data, token.lexpos)\n",
        "    message = \"Caracter ilegal '%s'\" % token.value[0]\n",
        "\n",
        "    # print(f\"[{file}]:[{line},{column}]: {message}.\") \n",
        "    print(message)\n",
        "\n",
        "    token.lexer.skip(1)\n",
        "\n",
        "    # token.lexer.has_error = Trueb"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9_fzwX8UWnf"
      },
      "source": [
        "def main():\n",
        "    argv[1] = 'teste.tpp'\n",
        "    aux = argv[1].split('.')\n",
        "    if aux[-1] != 'tpp':\n",
        "      raise IOError(\"Not a .tpp file!\")\n",
        "    data = open(argv[1])\n",
        "\n",
        "    source_file = data.read()\n",
        "    lexer.input(source_file)\n",
        "\n",
        "    # Tokenize\n",
        "    while True:\n",
        "      tok = lexer.token()\n",
        "      if not tok: \n",
        "        break      # No more input\n",
        "      # print(tok)\n",
        "      print(tok.type)\n",
        "      #print(tok.value)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "w7i4m7zoUcpc",
        "outputId": "209bb6b2-69ee-49bf-a807-6310c69d51da"
      },
      "source": [
        "# Build the lexer.\n",
        "__file__ = \"01-compiladores-analise-lexica-tpplex.ipynb\"\n",
        "lexer = lex.lex(optimize=False,debug=True,debuglog=log)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INTEIRO\nDOIS_PONTOS\nID\nABRE_COL\nNUM_INTEIRO\nFECHA_COL\nFLUTUANTE\nDOIS_PONTOS\nID\nINTEIRO\nID\nABRE_PAR\nINTEIRO\nDOIS_PONTOS\nID\nVIRGULA\nFLUTUANTE\nDOIS_PONTOS\nID\nFECHA_PAR\nINTEIRO\nDOIS_PONTOS\nID\nSE\nABRE_PAR\nID\nMAIOR\nID\nFECHA_PAR\nENTAO\nID\nATRIBUICAO\nID\nADICAO\nID\nSENAO\nID\nATRIBUICAO\nID\nMULTIPLICACAO\nID\nFIM\nRETORNA\nABRE_PAR\nID\nFECHA_PAR\nFIM\nID\nABRE_PAR\nINTEIRO\nDOIS_PONTOS\nID\nVIRGULA\nFLUTUANTE\nDOIS_PONTOS\nID\nFECHA_PAR\nID\nATRIBUICAO\nID\nID\nATRIBUICAO\nID\nFIM\nINTEIRO\nID\nABRE_PAR\nFECHA_PAR\nINTEIRO\nDOIS_PONTOS\nID\nVIRGULA\nID\nFLUTUANTE\nDOIS_PONTOS\nID\nID\nATRIBUICAO\nNUM_INTEIRO\nADICAO\nNUM_INTEIRO\nLEIA\nABRE_PAR\nID\nFECHA_PAR\nLEIA\nABRE_PAR\nID\nFECHA_PAR\nID\nATRIBUICAO\nNUM_PONTO_FLUTUANTE\nADICAO\nNUM_PONTO_FLUTUANTE\nID\nABRE_PAR\nNUM_INTEIRO\nVIRGULA\nNUM_PONTO_FLUTUANTE\nFECHA_PAR\nID\nATRIBUICAO\nID\nABRE_PAR\nID\nVIRGULA\nID\nFECHA_PAR\nESCREVA\nABRE_PAR\nID\nFECHA_PAR\nRETORNA\nABRE_PAR\nNUM_INTEIRO\nFECHA_PAR\nFIM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9UN81N7XBz_",
        "outputId": "d9ccc961-134c-4a23-bb49-ee8ee65b2c79"
      },
      "source": [
        "!python tpplex.py lexica-testes/teste-2.tpp"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LexToken(INTEIRO,'inteiro',1,0)\nLexToken(DOIS_PONTOS,':',1,7)\nLexToken(ID,'n',1,9)\nLexToken(VIRGULA,',',1,10)\nLexToken(ID,'g',1,12)\nLexToken(INTEIRO,'inteiro',3,15)\nLexToken(ID,'fatorial',3,23)\nLexToken(ABRE_PAR,'(',3,31)\nLexToken(INTEIRO,'inteiro',3,32)\nLexToken(DOIS_PONTOS,':',3,39)\nLexToken(ID,'n',3,41)\nLexToken(FECHA_PAR,')',3,42)\nLexToken(FLUTUANTE,'flutuante',4,45)\nLexToken(DOIS_PONTOS,':',4,54)\nLexToken(ID,'d',4,56)\nLexToken(ID,'d',5,59)\nLexToken(ATRIBUICAO,':=',5,61)\nLexToken(NUM_INTEIRO,'+5',5,64)\nLexToken(NUM_PONTO_FLUTUANTE,'.6',5,66)\nLexToken(INTEIRO,'inteiro',6,71)\nLexToken(DOIS_PONTOS,':',6,78)\nLexToken(ID,'fat',6,80)\nLexToken(SE,'se',7,85)\nLexToken(ID,'n',7,88)\nLexToken(MAIOR,'>',7,90)\nLexToken(NUM_INTEIRO,'10',7,92)\nLexToken(ENTAO,'então',7,95)\nLexToken(SE,'se',8,103)\nLexToken(ID,'n',8,106)\nLexToken(MAIOR,'>',8,108)\nLexToken(NUM_INTEIRO,'0',8,110)\nLexToken(ENTAO,'então',8,112)\nLexToken(ID,'fat',9,144)\nLexToken(ATRIBUICAO,':=',9,148)\nLexToken(NUM_INTEIRO,'1',9,151)\nLexToken(REPITA,'repita',10,156)\nLexToken(REPITA,'repita',11,167)\nLexToken(ID,'fat',12,179)\nLexToken(ATRIBUICAO,':=',12,183)\nLexToken(ID,'fat',12,186)\nLexToken(MULTIPLICACAO,'*',12,190)\nLexToken(ID,'n',12,192)\nLexToken(ATE,'até',13,198)\nLexToken(ID,'n',13,202)\nLexToken(IGUALDADE,'=',13,204)\nLexToken(NUM_INTEIRO,'0',13,206)\nLexToken(ID,'fat',14,212)\nLexToken(ATRIBUICAO,':=',14,216)\nLexToken(ID,'fat',14,219)\nLexToken(MULTIPLICACAO,'*',14,223)\nLexToken(ID,'n',14,225)\nLexToken(ATE,'até',15,230)\nLexToken(ID,'n',15,234)\nLexToken(IGUALDADE,'=',15,236)\nLexToken(NUM_INTEIRO,'0',15,238)\nLexToken(SENAO,'senão',16,242)\nLexToken(ID,'fat',17,251)\nLexToken(ATRIBUICAO,':=',17,255)\nLexToken(NUM_INTEIRO,'5',17,258)\nLexToken(FIM,'fim',18,262)\nLexToken(FIM,'fim',19,267)\nLexToken(INTEIRO,'inteiro',20,272)\nLexToken(DOIS_PONTOS,':',20,279)\nLexToken(ID,'teste',20,281)\nLexToken(ID,'teste',21,288)\nLexToken(ATRIBUICAO,':=',21,294)\nLexToken(ABRE_PAR,'(',21,297)\nLexToken(NUM_INTEIRO,'5',21,298)\nLexToken(NUM_INTEIRO,'+10',21,299)\nLexToken(FECHA_PAR,')',21,302)\nLexToken(MULTIPLICACAO,'*',21,303)\nLexToken(NUM_INTEIRO,'14',21,304)\nLexToken(ID,'teste',22,308)\nLexToken(ATRIBUICAO,':=',22,314)\nLexToken(NUM_INTEIRO,'5',22,317)\nLexToken(NUM_INTEIRO,'+10',22,318)\nLexToken(MULTIPLICACAO,'*',22,321)\nLexToken(NUM_INTEIRO,'14',22,322)\nLexToken(ID,'teste',23,326)\nLexToken(ATRIBUICAO,':=',23,332)\nLexToken(NUM_INTEIRO,'-5',23,335)\nLexToken(SUBTRACAO,'-',23,337)\nLexToken(ABRE_PAR,'(',23,338)\nLexToken(NUM_INTEIRO,'1',23,339)\nLexToken(NUM_INTEIRO,'+5',23,340)\nLexToken(FECHA_PAR,')',23,342)\nLexToken(ID,'teste',24,345)\nLexToken(ATRIBUICAO,':=',24,351)\nLexToken(NUM_INTEIRO,'5',24,354)\nLexToken(NUM_INTEIRO,'-1',24,355)\nLexToken(ID,'teste',25,359)\nLexToken(ATRIBUICAO,':=',25,365)\nLexToken(NUM_INTEIRO,'10',25,368)\nLexToken(MULTIPLICACAO,'*',25,370)\nLexToken(NUM_INTEIRO,'8',25,371)\nLexToken(FIM,'fim',26,373)\nLexToken(INTEIRO,'inteiro',28,378)\nLexToken(ID,'principal',28,386)\nLexToken(ABRE_PAR,'(',28,395)\nLexToken(FECHA_PAR,')',28,396)\nLexToken(LEIA,'leia',29,399)\nLexToken(ABRE_PAR,'(',29,403)\nLexToken(ID,'n',29,404)\nLexToken(FECHA_PAR,')',29,405)\nLexToken(ESCREVA,'escreva',30,408)\nLexToken(ABRE_PAR,'(',30,415)\nLexToken(ID,'fatorial',30,416)\nLexToken(ABRE_PAR,'(',30,424)\nLexToken(ID,'fatorial',30,425)\nLexToken(ABRE_PAR,'(',30,433)\nLexToken(NUM_INTEIRO,'1',30,434)\nLexToken(FECHA_PAR,')',30,435)\nLexToken(FECHA_PAR,')',30,436)\nLexToken(FECHA_PAR,')',30,437)\nLexToken(FIM,'fim',31,439)\nLexToken(NUM_INTEIRO,'1',33,444)\nLexToken(NUM_INTEIRO,'+2',33,445)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}